{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-01-12 / FMA sub-sampling\n",
    "\n",
    "* Problem statement:\n",
    "  * Input:\n",
    "    * `C` csv files\n",
    "    * each file has `n` rows. Each row in file `c` encodes the prediction for class `c` on a 1sec segment.\n",
    "    * A target number `k`\n",
    "    * Target fractions for class representations `p[c]`.\n",
    "    \n",
    "  * Output:\n",
    "    * A set of `k` clips, each 10 seconds in duration\n",
    "    * Aggregate predicted likelihoods for each class `c` on each clip `k`\n",
    "    * Each class `c` has aggregate likelihood at least `p[c] * k`\n",
    "\n",
    "\n",
    "* Method:\n",
    "  1. drop edge effects from the beginning and end of tracks: remove the first and last frames from each track.\n",
    "  2. window the frame observations into 10sec clips with aggregate labels\n",
    "  3. threshold the aggregate likelihoods to binarize the representation\n",
    "  4. subsample the 10sec clips using entrofy\n",
    "\n",
    "\n",
    "* Questions:\n",
    "  * How should likelihoods be aggregated within a segment?\n",
    "    * Mean?  Max?  Quartile?\n",
    "    * Mean makes sense from the perspective of random frame sampling\n",
    "    * Quartile makes sense wrt sparse events\n",
    "    * Max makes sense wrt extremely sparse events\n",
    "  * How should likelihoods be thresholded?  0.5?  Empirical average over X?\n",
    "    * $p[y] = \\sum_x p[y|x] * p[x] \\approx \\sum_{x \\in X} p[y|x] /|X| $\n",
    "    * But that doesn't matter really.  Threshold should be bayes optimal (=> 0.5)\n",
    "  * What's the target number of positives per class `k * p[c]`? \n",
    "    * Maybe that should be determined by the base rate estimation `p[y]`?\n",
    "  \n",
    "  \n",
    "* Next step: Question scheduling on CF.\n",
    "  * Idea: cluster the tracks according to aggregated likelihood vectors\n",
    "    * Or maybe by their thresholded likelihoods?\n",
    "  * Set the number of clusters to be relatively large (say, 23^2 ~= 512)\n",
    "  * When generating questions for an annotator, assign them to a cluster and only generate questions from that cluster\n",
    "  * Reasoning: this will keep the labels consistent from one question to the next\n",
    "  \n",
    "  \n",
    "* UPDATE:\n",
    "  * Windowing and aggregation is happening upstream of this\n",
    "  * Aggregation is max over the middle 8 frames\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-01-19\n",
    "\n",
    "* Eric has provided the per-fragment aggregated estimates as one giant table\n",
    "* So what are our entrofy parameters?\n",
    "  * attribute thresholds\n",
    "      * Do we only do <>0.5? \n",
    "      * Or break likelihood into quartiles?\n",
    "      * **Sounds like quartiles are the way to go**\n",
    "  * target proportions per class?\n",
    "      * we can try to preserve the empirical distribution\n",
    "      * or a biased distribution achieved by grouping on the track ids?\n",
    "      * or uniform?\n",
    "      * **Uniform across quartiles for each instrument**\n",
    "  * output set size?\n",
    "      * 20-50 positives per instrument?\n",
    "      * say, `16 * 4 * n_classes`\n",
    "      * Maybe round up to 1K to start\n",
    "  \n",
    "* If we only want one example per track, we can make an aux categorical column that's the track index, and set the target number to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-02-02\n",
    "\n",
    "* Turns out we didn't get the data transferred in time on 01/19, so still waiting\n",
    "* output set size: 500-1000 positives per class\n",
    "* try both hard threshold and quartile sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import entrofy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappers = {col: entrofy.mappers.ContinuousMapper(df[col], n_out=4,\n",
    "                                                 boundaries=[0.0, 0.25, 0.5, 0.75, 1.0]) for col in df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, score = entrofy.entrofy(df, 1000, mappers=mappers, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
